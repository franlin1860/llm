{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franlin1860/llm/blob/main/ingestion_nodes_20240827.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c676db",
      "metadata": {
        "id": "57c676db"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/docs/examples/low_level/ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c919f307-07b1-41bd-bc5d-51edd8677983",
      "metadata": {
        "id": "c919f307-07b1-41bd-bc5d-51edd8677983"
      },
      "source": [
        "# Building Data Ingestion from Scratch\n",
        "\n",
        "In this tutorial, we show you how to build a data ingestion pipeline into a vector database.\n",
        "\n",
        "We use Pinecone as the vector database.\n",
        "\n",
        "We will show how to do the following:\n",
        "1. How to load in documents.\n",
        "2. How to use a text splitter to split documents.\n",
        "3. How to **manually** construct nodes from each text chunk.\n",
        "4. [Optional] Add metadata to each Node.\n",
        "5. How to generate embeddings for each text chunk.\n",
        "6. How to insert into a vector database."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer: https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_nodes/"
      ],
      "metadata": {
        "id": "On1qm867Bhla"
      },
      "id": "On1qm867Bhla"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prevent disconnection"
      ],
      "metadata": {
        "id": "oec_fTCtvxGi"
      },
      "id": "oec_fTCtvxGi"
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown <h3>â† è¾“å…¥äº†ä»£ç åè¿è¡Œä»¥é˜²æ­¢æ–­å¼€</h>\n",
        "import IPython\n",
        "from google.colab import output\n",
        "\n",
        "display(IPython.display.Javascript('''\n",
        " function ClickConnect(){\n",
        "   btn = document.querySelector(\"colab-connect-button\")\n",
        "   if (btn != null){\n",
        "     console.log(\"Click colab-connect-button\");\n",
        "     btn.click()\n",
        "     }\n",
        "\n",
        "   btn = document.getElementById('ok')\n",
        "   if (btn != null){\n",
        "     console.log(\"Click reconnect\");\n",
        "     btn.click()\n",
        "     }\n",
        "  }\n",
        "\n",
        "setInterval(ClickConnect,60000)\n",
        "'''))\n",
        "\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "c5uEByFCv3ft",
        "outputId": "4cd051ba-d440-4a97-8453-4f23d9cd4e4a"
      },
      "id": "c5uEByFCv3ft",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              " function ClickConnect(){\n",
              "   btn = document.querySelector(\"colab-connect-button\")\n",
              "   if (btn != null){\n",
              "     console.log(\"Click colab-connect-button\");\n",
              "     btn.click()\n",
              "     }\n",
              "\n",
              "   btn = document.getElementById('ok')\n",
              "   if (btn != null){\n",
              "     console.log(\"Click reconnect\");\n",
              "     btn.click()\n",
              "     }\n",
              "  }\n",
              "\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function ConnectButton(){\n",
        "    console.log(\"Connect pushed\");\n",
        "    document.querySelector(\"#connect\").click()\n",
        "}\n",
        "setInterval(ConnectButton,60000);"
      ],
      "metadata": {
        "id": "pjBTdO9tv_6E"
      },
      "id": "pjBTdO9tv_6E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "92b20306",
      "metadata": {
        "id": "92b20306"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ğŸ¦™."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b60e707a",
      "metadata": {
        "id": "b60e707a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e0b7359d-4f98-4944-9dc7-58eb9deaa77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.11.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-agent-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.3.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-cli<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.3.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.12.0,>=0.11.1 (from llama-index)\n",
            "  Downloading llama_index_core-0.11.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting llama-index-embeddings-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl.metadata (635 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.2.0-py3-none-any.whl.metadata (648 bytes)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl.metadata (728 bytes)\n",
            "Collecting llama-index-program-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.2.0-py3-none-any.whl.metadata (766 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting llama-index-readers-file<0.3.0,>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.2.0-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting nltk>3.8.1 (from llama-index)\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (3.10.5)\n",
            "Collecting dataclasses-json (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (2024.6.1)\n",
            "Collecting httpx (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (3.3)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.2.0 (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.1->llama-index) (1.16.0)\n",
            "Collecting llama-cloud>=0.0.11 (from llama-index-indices-managed-llama-cloud>=0.3.0->llama-index)\n",
            "  Downloading llama_cloud-0.0.15-py3-none-any.whl.metadata (751 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.1.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.3.0,>=0.2.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.2.0->llama-index)\n",
            "  Downloading llama_parse-0.5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.1->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.3.0,>=0.2.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.14.0->llama-index-agent-openai<0.4.0,>=0.3.0->llama-index)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.1->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.1->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.12.0,>=0.11.1->llama-index)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.1->llama-index) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.12.0,>=0.11.1->llama-index) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.16.0)\n",
            "Downloading llama_index-0.11.1-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.3.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.11.1-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.2.3-py3-none-any.whl (6.3 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.3.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post3-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.2.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.2.0-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.2.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.2.0-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.2.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading llama_cloud-0.0.15-py3-none-any.whl (180 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.2/180.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: striprtf, dirtyjson, tenacity, pypdf, nltk, mypy-extensions, marshmallow, jiter, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-index-core, llama-cloud, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-legacy, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0 llama-cloud-0.0.15 llama-index-0.11.1 llama-index-agent-openai-0.3.0 llama-index-cli-0.3.0 llama-index-core-0.11.1 llama-index-embeddings-openai-0.2.3 llama-index-indices-managed-llama-cloud-0.3.0 llama-index-legacy-0.9.48.post3 llama-index-llms-openai-0.2.0 llama-index-multi-modal-llms-openai-0.2.0 llama-index-program-openai-0.2.0 llama-index-question-gen-openai-0.2.0 llama-index-readers-file-0.2.0 llama-index-readers-llama-parse-0.2.0 llama-parse-0.5.0 marshmallow-3.22.0 mypy-extensions-1.0.0 nltk-3.9.1 openai-1.42.0 pypdf-4.3.1 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22fb9e0a-566b-4f34-b9cf-72193cb51adb",
      "metadata": {
        "id": "22fb9e0a-566b-4f34-b9cf-72193cb51adb"
      },
      "source": [
        "## LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HPwWNeZwgwE8",
      "metadata": {
        "id": "HPwWNeZwgwE8"
      },
      "source": [
        "## Environment\n",
        "\n",
        "First we add our dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bCwZFn6_iAR1",
      "metadata": {
        "id": "bCwZFn6_iAR1"
      },
      "source": [
        "#### Set Environment Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "M1l2emfWgjgE",
      "metadata": {
        "id": "M1l2emfWgjgE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PWMbn7GooMm5",
      "metadata": {
        "id": "PWMbn7GooMm5"
      },
      "source": [
        "Set your LLM api key, and environment in the file we created."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb486eb-c0b8-40e2-9038-da97aef63139",
      "metadata": {
        "id": "bcb486eb-c0b8-40e2-9038-da97aef63139"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index-llms-openai_like\n",
        "!pip install llama_index-embeddings-huggingface"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2YyxwD6By_eK",
        "outputId": "4eaf6804-c386-4cfd-c855-81b6bf01558f"
      },
      "id": "2YyxwD6By_eK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_index-llms-openai_like\n",
            "  Downloading llama_index_llms_openai_like-0.2.0-py3-none-any.whl.metadata (753 bytes)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama_index-llms-openai_like) (0.11.1)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index-llms-openai_like) (0.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama_index-llms-openai_like) (4.42.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2024.6.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2.8.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.16.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.40.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-openai<0.3.0,>=0.2.0->llama_index-llms-openai_like) (1.42.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama_index-llms-openai_like) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama_index-llms-openai_like) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama_index-llms-openai_like) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama_index-llms-openai_like) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama_index-llms-openai_like) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.37.0->llama_index-llms-openai_like) (0.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama_index-llms-openai_like) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama_index-llms-openai_like) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama_index-llms-openai_like) (0.5.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama_index-llms-openai_like) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (3.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama_index-llms-openai_like) (3.22.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->llama-index-llms-openai<0.3.0,>=0.2.0->llama_index-llms-openai_like) (1.2.2)\n",
            "Downloading llama_index_llms_openai_like-0.2.0-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: llama_index-llms-openai_like\n",
            "Successfully installed llama_index-llms-openai_like-0.2.0\n",
            "Collecting llama_index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl.metadata (718 bytes)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (0.23.5)\n",
            "Requirement already satisfied: llama-index-core<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from llama_index-embeddings-huggingface) (0.11.1)\n",
            "Collecting sentence-transformers>=2.6.1 (from llama_index-embeddings-huggingface)\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (3.10.5)\n",
            "Collecting minijinja>=1.0 (from huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface)\n",
            "  Downloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (2.0.32)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (0.27.0)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (9.4.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (4.42.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (2.4.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (4.0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (2024.5.15)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.0.0->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama_index-embeddings-huggingface) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (3.1.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (0.19.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (3.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (0.14.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.12.0,>=0.11.0->llama_index-embeddings-huggingface) (1.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.1->llama_index-embeddings-huggingface) (1.3.0)\n",
            "Downloading llama_index_embeddings_huggingface-0.3.1-py3-none-any.whl (8.6 kB)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading minijinja-2.2.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (861 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m861.9/861.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: minijinja, sentence-transformers, llama_index-embeddings-huggingface\n",
            "Successfully installed llama_index-embeddings-huggingface-0.3.1 minijinja-2.2.0 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from llama_index.llms.openai_like import OpenAILike\n",
        "from llama_index.core import Settings, ServiceContext\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "# é…ç½®æ—¥å¿—\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
        "\n",
        "# å®šä¹‰DeepSpeed model\n",
        "llm = OpenAILike(model=\"deepseek-chat\",\n",
        "                 api_base=\"https://api.deepseek.com/v1\",\n",
        "                 api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
        "                 temperature=0.6,\n",
        "                 is_chat_model=True)\n",
        "\n",
        "# é…ç½®ç¯å¢ƒ\n",
        "Settings.llm = llm\n",
        "\n",
        "# è®¾ç½®åµŒå…¥æ¨¡å‹\n",
        "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 256"
      ],
      "metadata": {
        "id": "Eqn6rjP7zHvl"
      },
      "id": "Eqn6rjP7zHvl",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = llm.complete(\"ä½ å¥½\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CJk5Bw662IY",
        "outputId": "f8e58fdf-b9cf-4595-ccfb-437dc290d0d6"
      },
      "id": "0CJk5Bw662IY",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä½ å¥½ï¼æ¬¢è¿ä½¿ç”¨äººå·¥æ™ºèƒ½åŠ©æ‰‹ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9437a0-3d52-4586-8217-43944971a2cc",
      "metadata": {
        "id": "be9437a0-3d52-4586-8217-43944971a2cc"
      },
      "source": [
        "## Build an Ingestion Pipeline from Scratch\n",
        "\n",
        "We show how to build an ingestion pipeline as mentioned in the introduction.\n",
        "\n",
        "Note that steps (2) and (3) can be handled via our `NodeParser` abstractions, which handle splitting and node creation.\n",
        "\n",
        "For the purposes of this tutorial, we show you how to create these objects manually."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1c9630-2a6b-4656-b272-de1b869c8977",
      "metadata": {
        "id": "6d1c9630-2a6b-4656-b272-de1b869c8977"
      },
      "source": [
        "### 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "48739cfc-c05a-420a-8c78-280892f8d7a0",
      "metadata": {
        "id": "48739cfc-c05a-420a-8c78-280892f8d7a0",
        "outputId": "7400debb-3db5-47ee-c563-33607459bfa8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-27 07:50:06--  https://arxiv.org/pdf/2307.09288.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.3.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2307.09288 [following]\n",
            "--2024-08-27 07:50:07--  http://arxiv.org/pdf/2307.09288\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13661300 (13M) [application/pdf]\n",
            "Saving to: â€˜data/llama2.pdfâ€™\n",
            "\n",
            "data/llama2.pdf     100%[===================>]  13.03M  85.4MB/s    in 0.2s    \n",
            "\n",
            "2024-08-27 07:50:07 (85.4 MB/s) - â€˜data/llama2.pdfâ€™ saved [13661300/13661300]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdE5tLHK0Nx1",
        "outputId": "96f90485-0d2d-4b66-a658-cad7db27c7fb"
      },
      "id": "VdE5tLHK0Nx1",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.9 pymupdf-1.24.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "079666c5-0685-413d-a765-17f71ae89506",
      "metadata": {
        "id": "079666c5-0685-413d-a765-17f71ae89506"
      },
      "outputs": [],
      "source": [
        "import fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4eee7692-2188-4552-9f2e-cb90ac6b7678",
      "metadata": {
        "id": "4eee7692-2188-4552-9f2e-cb90ac6b7678"
      },
      "outputs": [],
      "source": [
        "file_path = \"./data/llama2.pdf\"\n",
        "doc = fitz.open(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c573db-1863-45c3-9049-8bad535e6e35",
      "metadata": {
        "id": "74c573db-1863-45c3-9049-8bad535e6e35"
      },
      "source": [
        "### 2. Use a Text Splitter to Split Documents\n",
        "\n",
        "Here we import our `SentenceSplitter` to split document texts into smaller chunks, while preserving paragraphs/sentences as much as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9e175007-84d5-406e-bf5f-6ecacfbfd152",
      "metadata": {
        "id": "9e175007-84d5-406e-bf5f-6ecacfbfd152"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "0dbccb26-ea2a-48c9-adb4-1ebe88adaa1c",
      "metadata": {
        "id": "0dbccb26-ea2a-48c9-adb4-1ebe88adaa1c"
      },
      "outputs": [],
      "source": [
        "text_parser = SentenceSplitter(\n",
        "    chunk_size=1024,\n",
        "    # separator=\" \",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7a9bed96-adfa-40c9-92bd-9dba68d58730",
      "metadata": {
        "id": "7a9bed96-adfa-40c9-92bd-9dba68d58730"
      },
      "outputs": [],
      "source": [
        "text_chunks = []\n",
        "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
        "doc_idxs = []\n",
        "for doc_idx, page in enumerate(doc):\n",
        "    page_text = page.get_text(\"text\")\n",
        "    cur_text_chunks = text_parser.split_text(page_text)\n",
        "    text_chunks.extend(cur_text_chunks)\n",
        "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354157d6-b436-4f0a-bf6e-f0a197e54c60",
      "metadata": {
        "id": "354157d6-b436-4f0a-bf6e-f0a197e54c60"
      },
      "source": [
        "### 3. Manually Construct Nodes from Text Chunks\n",
        "\n",
        "We convert each chunk into a `TextNode` object, a low-level data abstraction in LlamaIndex that stores content but also allows defining metadata + relationships with other Nodes.\n",
        "\n",
        "We inject metadata from the document into each node.\n",
        "\n",
        "This essentially replicates logic in our `SentenceSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "33b93044-3eb4-4c77-bc40-be53dffd3749",
      "metadata": {
        "id": "33b93044-3eb4-4c77-bc40-be53dffd3749"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.schema import TextNode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "adbfcb3f-5554-4594-ae80-7236e28485aa",
      "metadata": {
        "id": "adbfcb3f-5554-4594-ae80-7236e28485aa"
      },
      "outputs": [],
      "source": [
        "nodes = []\n",
        "for idx, text_chunk in enumerate(text_chunks):\n",
        "    node = TextNode(\n",
        "        text=text_chunk,\n",
        "    )\n",
        "    src_doc_idx = doc_idxs[idx]\n",
        "    src_page = doc[src_doc_idx]\n",
        "    nodes.append(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3iidPVIiwYUg",
      "metadata": {
        "id": "3iidPVIiwYUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e88d756-c188-435a-9295-fc3aedf43dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ],
      "source": [
        "print(nodes[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "257bb2e3-608a-4542-ba29-f29b59771a3f",
      "metadata": {
        "id": "257bb2e3-608a-4542-ba29-f29b59771a3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "68833f03-4a15-42c0-999f-6c769c22fa20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
            "Hugo Touvronâˆ—\n",
            "Louis Martinâ€ \n",
            "Kevin Stoneâ€ \n",
            "Peter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\n",
            "Prajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\n",
            "Guillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\n",
            "Cynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\n",
            "Hakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\n",
            "Punit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\n",
            "Yinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\n",
            "Igor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\n",
            "Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
            "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
            "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
            "Sergey Edunov\n",
            "Thomas Scialomâˆ—\n",
            "GenAI, Meta\n",
            "Abstract\n",
            "In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\n",
            "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
            "Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our\n",
            "models outperform open-source chat models on most benchmarks we tested, and based on\n",
            "our human evaluations for helpfulness and safety, may be a suitable substitute for closed-\n",
            "source models. We provide a detailed description of our approach to fine-tuning and safety\n",
            "improvements of Llama 2-Chat in order to enable the community to build on our work and\n",
            "contribute to the responsible development of LLMs.\n",
            "âˆ—Equal contribution, corresponding authors: {tscialom, htouvron}@meta.com\n",
            "â€ Second author\n",
            "Contributions for all the authors can be found in Section A.1.\n",
            "arXiv:2307.09288v2  [cs.CL]  19 Jul 2023\n"
          ]
        }
      ],
      "source": [
        "# print a sample node\n",
        "print(nodes[0].get_content(metadata_mode=\"all\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac468f5-870c-4486-b576-2aa6d9eaf322",
      "metadata": {
        "id": "fac468f5-870c-4486-b576-2aa6d9eaf322"
      },
      "source": [
        "###  4. Extract Metadata from each Node\n",
        "\n",
        "We extract metadata from each Node using our Metadata extractors.\n",
        "\n",
        "This will add more metadata to each Node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "9c369188-9cc9-4550-924e-b29d212ad057",
      "metadata": {
        "id": "9c369188-9cc9-4550-924e-b29d212ad057"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.extractors import (\n",
        "    QuestionsAnsweredExtractor,\n",
        "    TitleExtractor,\n",
        ")\n",
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "\n",
        "llm = Settings.llm\n",
        "\n",
        "extractors = [\n",
        "    TitleExtractor(nodes=5, llm=llm),\n",
        "    QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f5501ffc-9bbb-4b48-9181-4e4e371e8f41",
      "metadata": {
        "id": "f5501ffc-9bbb-4b48-9181-4e4e371e8f41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9fea42fd-2d71-48ef-e11b-51e09a4edf4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|â–ˆâ–ˆ        | 1/5 [00:04<00:16,  4.12s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:07<00:10,  3.47s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [00:07<00:04,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:15<00:04,  4.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:24<00:00,  4.99s/it]\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  1%|          | 1/107 [00:26<46:17, 26.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  2%|â–         | 2/107 [00:40<33:26, 19.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  3%|â–         | 3/107 [00:50<25:55, 14.96s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  4%|â–         | 4/107 [00:53<17:56, 10.45s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  5%|â–         | 5/107 [00:56<12:45,  7.51s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  6%|â–Œ         | 6/107 [00:59<10:00,  5.95s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|â–‹         | 7/107 [01:01<08:11,  4.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  7%|â–‹         | 8/107 [01:09<09:21,  5.67s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  8%|â–Š         | 9/107 [01:10<07:17,  4.46s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "  9%|â–‰         | 10/107 [01:11<05:09,  3.19s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 10%|â–ˆ         | 11/107 [01:17<06:21,  3.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 11%|â–ˆ         | 12/107 [01:22<06:46,  4.28s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 12%|â–ˆâ–        | 13/107 [01:24<05:39,  3.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 13%|â–ˆâ–        | 14/107 [01:27<05:40,  3.66s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 14%|â–ˆâ–        | 15/107 [01:30<05:16,  3.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 15%|â–ˆâ–        | 16/107 [01:34<05:31,  3.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 16%|â–ˆâ–Œ        | 17/107 [01:38<05:18,  3.53s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 17%|â–ˆâ–‹        | 18/107 [01:42<05:44,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 18%|â–ˆâ–Š        | 19/107 [01:46<05:43,  3.91s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 19%|â–ˆâ–Š        | 20/107 [01:50<05:37,  3.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 20%|â–ˆâ–‰        | 21/107 [01:51<04:15,  2.97s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|â–ˆâ–ˆ        | 22/107 [01:53<03:36,  2.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 21%|â–ˆâ–ˆâ–       | 23/107 [02:05<07:46,  5.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 23%|â–ˆâ–ˆâ–       | 25/107 [02:06<04:23,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 24%|â–ˆâ–ˆâ–       | 26/107 [02:08<03:44,  2.77s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 25%|â–ˆâ–ˆâ–Œ       | 27/107 [02:15<05:21,  4.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 26%|â–ˆâ–ˆâ–Œ       | 28/107 [02:18<04:54,  3.73s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 27%|â–ˆâ–ˆâ–‹       | 29/107 [02:18<03:34,  2.75s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 28%|â–ˆâ–ˆâ–Š       | 30/107 [02:18<02:34,  2.00s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 29%|â–ˆâ–ˆâ–‰       | 31/107 [02:27<04:54,  3.88s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 30%|â–ˆâ–ˆâ–‰       | 32/107 [02:30<04:35,  3.67s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 31%|â–ˆâ–ˆâ–ˆ       | 33/107 [02:33<04:06,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 32%|â–ˆâ–ˆâ–ˆâ–      | 34/107 [02:35<03:41,  3.04s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 33%|â–ˆâ–ˆâ–ˆâ–      | 35/107 [02:41<04:40,  3.89s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 34%|â–ˆâ–ˆâ–ˆâ–      | 36/107 [02:44<04:23,  3.71s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 35%|â–ˆâ–ˆâ–ˆâ–      | 37/107 [02:50<05:07,  4.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 38/107 [02:54<04:43,  4.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 36%|â–ˆâ–ˆâ–ˆâ–‹      | 39/107 [02:54<03:27,  3.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 40/107 [02:57<03:17,  2.95s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 41/107 [03:03<04:11,  3.81s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 42/107 [03:06<03:54,  3.61s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 43/107 [03:13<04:59,  4.68s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 44/107 [03:16<04:23,  4.19s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 45/107 [03:18<03:40,  3.56s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 46/107 [03:20<03:09,  3.11s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 47/107 [03:30<05:13,  5.23s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 48/107 [03:31<03:41,  3.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 49/107 [03:33<03:08,  3.25s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 50/107 [03:34<02:25,  2.55s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 51/107 [03:44<04:30,  4.84s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 52/107 [03:45<03:22,  3.68s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 53/107 [03:46<02:42,  3.01s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 54/107 [03:48<02:23,  2.71s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 55/107 [03:56<03:32,  4.08s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 56/107 [03:58<03:02,  3.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 57/107 [03:59<02:17,  2.76s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 58/107 [03:59<01:44,  2.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 59/107 [04:11<03:50,  4.80s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 60/107 [04:11<02:41,  3.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 61/107 [04:13<02:27,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 62/107 [04:14<01:52,  2.50s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 63/107 [04:22<03:02,  4.14s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 64/107 [04:25<02:45,  3.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 65/107 [04:27<02:06,  3.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 66/107 [04:28<01:40,  2.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 67/107 [04:37<02:59,  4.49s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 68/107 [04:38<02:18,  3.56s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 69/107 [04:42<02:13,  3.52s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 70/107 [04:42<01:37,  2.65s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 71/107 [04:49<02:23,  3.98s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 72/107 [04:54<02:23,  4.10s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 73/107 [04:54<01:39,  2.93s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 74/107 [04:58<01:43,  3.12s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 75/107 [05:01<01:46,  3.34s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 76/107 [05:08<02:15,  4.38s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 77/107 [05:12<02:01,  4.06s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 78/107 [05:13<01:36,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 79/107 [05:20<02:02,  4.39s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 80/107 [05:24<01:52,  4.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 81/107 [05:26<01:34,  3.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 82/107 [05:30<01:36,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 83/107 [05:35<01:39,  4.16s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 84/107 [05:39<01:30,  3.94s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 85/107 [05:42<01:20,  3.64s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 86/107 [05:44<01:06,  3.15s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 87/107 [05:52<01:32,  4.63s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 88/107 [05:53<01:07,  3.54s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 89/107 [05:54<00:50,  2.82s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 90/107 [05:58<00:56,  3.33s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 91/107 [06:06<01:13,  4.58s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 92/107 [06:08<00:57,  3.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 94/107 [06:10<00:32,  2.49s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 95/107 [06:19<00:49,  4.13s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 96/107 [06:22<00:42,  3.86s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 98/107 [06:23<00:21,  2.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 99/107 [06:32<00:32,  4.02s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 100/107 [06:36<00:27,  3.87s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 101/107 [06:37<00:19,  3.21s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 102/107 [06:41<00:17,  3.44s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 103/107 [06:48<00:17,  4.29s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 104/107 [06:52<00:12,  4.30s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 105/107 [06:55<00:07,  3.85s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 106/107 [06:58<00:03,  3.59s/it]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107/107 [07:02<00:00,  3.95s/it]\n"
          ]
        }
      ],
      "source": [
        "pipeline = IngestionPipeline(\n",
        "    transformations=extractors,\n",
        ")\n",
        "nodes = await pipeline.arun(nodes=nodes, in_place=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "WgbKmXr3ytPf",
      "metadata": {
        "id": "WgbKmXr3ytPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0909eab-2ff2-4875-b33d-b0077db9d1a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'document_title': '\"Llama 2: Comprehensive Insights into Pretraining, Fine-Tuning, Safety, Ethical Considerations, and Open-Source Contributions for Advanced Dialogue Optimization\"', 'questions_this_excerpt_can_answer': '1. **What is the range of parameter sizes for the Llama 2 collection of large language models?**\\n   - The excerpt specifies that Llama 2 includes large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\n\\n2. **How do the fine-tuned Llama 2-Chat models perform compared to other open-source chat models on benchmarks?**\\n   - The document states that the fine-tuned Llama 2-Chat models outperform open-source chat models on most benchmarks tested.\\n\\n3. **What are the primary objectives of the Llama 2-Chat models in terms of their application and safety?**\\n   - The abstract mentions that Llama 2-Chat models are optimized for dialogue use cases and are evaluated for helpfulness and safety, potentially serving as a suitable substitute for closed-source models. Additionally, the document details the approach to fine-tuning and safety improvements, aiming to enable community contributions to the responsible development of LLMs.'}\n"
          ]
        }
      ],
      "source": [
        "print(nodes[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d52522c-ffee-49d1-8651-658d70248053",
      "metadata": {
        "id": "9d52522c-ffee-49d1-8651-658d70248053"
      },
      "source": [
        "### 5. Generate Embeddings for each Node\n",
        "\n",
        "Generate document embeddings for each Node using our OpenAI embedding model (`text-embedding-ada-002`).\n",
        "\n",
        "Store these on the `embedding` property on each Node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6e071e36-e609-4a0c-a478-e8cfbe751cff",
      "metadata": {
        "id": "6e071e36-e609-4a0c-a478-e8cfbe751cff"
      },
      "outputs": [],
      "source": [
        "embed_model = Settings.embed_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "2047ca46-729f-4c5a-a8d7-3bc860604333",
      "metadata": {
        "id": "2047ca46-729f-4c5a-a8d7-3bc860604333"
      },
      "outputs": [],
      "source": [
        "for node in nodes:\n",
        "    node_embedding = embed_model.get_text_embedding(\n",
        "        node.get_content(metadata_mode=\"all\")\n",
        "    )\n",
        "    node.embedding = node_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f78014-dcca-43f5-95cb-9cfb5140b30e",
      "metadata": {
        "id": "11f78014-dcca-43f5-95cb-9cfb5140b30e"
      },
      "source": [
        "### 6. Load Nodes into a Vector Store\n",
        "\n",
        "We now insert these nodes into our `PineconeVectorStore`.\n",
        "\n",
        "**NOTE**: We skip the VectorStoreIndex abstraction, which is a higher-level abstraction that handles ingestion as well. We use `VectorStoreIndex` in the next section to fast-track retrieval/querying."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e7f8bd-d92a-40ad-8d9b-a18c04ddfca9",
      "metadata": {
        "id": "74e7f8bd-d92a-40ad-8d9b-a18c04ddfca9"
      },
      "source": [
        "## Retrieve and Query from the Vector Store\n",
        "\n",
        "Now that our ingestion is complete, we can retrieve/query this vector store.\n",
        "\n",
        "**NOTE**: We can use our high-level `VectorStoreIndex` abstraction here. See the next section to see how to define retrieval at a lower-level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "be6a4fe1-2665-43e6-a872-8e631e31b0fd",
      "metadata": {
        "id": "be6a4fe1-2665-43e6-a872-8e631e31b0fd"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import StorageContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "0e9d2495-d4f7-469a-9cea-a5cfc401c085",
      "metadata": {
        "id": "0e9d2495-d4f7-469a-9cea-a5cfc401c085"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex(nodes=nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "5c89e1c1-8ed1-45f5-b2a4-7c3382195693",
      "metadata": {
        "id": "5c89e1c1-8ed1-45f5-b2a4-7c3382195693"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "cd6e0ddb-97c9-4f42-8843-a36a29ba3f17",
      "metadata": {
        "id": "cd6e0ddb-97c9-4f42-8843-a36a29ba3f17"
      },
      "outputs": [],
      "source": [
        "query_str = \"Can you tell me about the key concepts for safety finetuning\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "950cae37-7bad-44a3-be51-4154a8630818",
      "metadata": {
        "id": "950cae37-7bad-44a3-be51-4154a8630818"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "c0b309bb-ca5a-4b15-948c-687038361c91",
      "metadata": {
        "id": "c0b309bb-ca5a-4b15-948c-687038361c91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8ef30a-db33-4651-a913-efb0e2b29cb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The key concepts for safety fine-tuning in Llama 2 include:\n",
            "\n",
            "1. **Supervised Safety Fine-Tuning**: This involves gathering adversarial prompts and safe demonstrations to teach the model to align with safety guidelines before integrating human feedback.\n",
            "\n",
            "2. **Safety RLHF (Reinforcement Learning from Human Feedback)**: This technique integrates safety into the RLHF pipeline, including training a safety-specific reward model and gathering more challenging adversarial prompts for fine-tuning.\n",
            "\n",
            "3. **Safety Context Distillation**: This method refines the RLHF pipeline by generating safer model responses using safety preprompts, such as \"You are a safe and responsible assistant,\" and then fine-tuning the model on these responses.\n",
            "\n",
            "4. **Safety Categories and Annotation Guidelines**: These guidelines help in creating adversarial prompts based on risk categories (illicit and criminal activities, hateful and harmful activities, unqualified advice) and attack vectors to cover different varieties of prompts that could elicit unsafe model behaviors.\n",
            "\n",
            "These techniques collectively aim to enhance the safety and reliability of the model's responses to various prompts.\n"
          ]
        }
      ],
      "source": [
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "llama_index_v2",
      "language": "python",
      "name": "llama_index_v2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}